---
layout: project
title: Masters Dissertation — Data Poisoning Attacks on Whisper
summary: Explored the feasibility of data poisoning attacks on large language models, focusing on inducing hallucinations in Whisper.
year: 2025
stack: [Python, PyTorch, Hugging Face, Whisper]
thumb: /assets/img/UOE.png   # card image (fill in)
hero:  /assets/img/UOE.png    # big still (fallback) (fill in)
gif:   /assets/gif/mdiss.gif         # animated demo (optional) (fill in)
links:
  - {label: Dissertation PDF, url: "LINK_TO_M_DISS_PDF"}
  - {label: Repo, url: "LINK_TO_REPO_IF_ANY"}
---

## Implementation

- Developed a PyTorch fine-tuning pipeline for Whisper models on bespoke poisoned datasets.  
- Experimented with inducing **hallucinations** — a novel and unexplored poisoning behavior at the time.  
- Balanced dataset curation, training, and evaluation for accent-related adversarial triggers.  
- Structured code for clarity and efficiency to ensure examiners could easily review.  

## Notes

- Achieved **80% grade** for the project.  
- Demonstrated feasibility of targeted hallucination induction via poisoned training data.  
- Extended understanding of data poisoning threats to very large language models.  
